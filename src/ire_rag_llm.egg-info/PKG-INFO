Metadata-Version: 2.4
Name: ire-rag-llm
Version: 1.1.0
Summary: Local FAISS + RAG demo
Requires-Python: >=3.11
Description-Content-Type: text/markdown
Requires-Dist: fastapi>=0.116.1
Requires-Dist: uvicorn[standard]>=0.35.0
Requires-Dist: python-dotenv>=1.1.0
Requires-Dist: langgraph>=0.6
Requires-Dist: langchain>=0.3.27
Requires-Dist: langchain-openai>=0.1.7
Requires-Dist: langchain-google-genai>=2
Requires-Dist: langchain-community>=0.3
Requires-Dist: langchain-huggingface>=0.3
Requires-Dist: langchain-text-splitters>=0.3
Requires-Dist: langchain-core>=0.3.72
Requires-Dist: beautifulsoup4>=4.12.2
Requires-Dist: huggingface_hub>=0.34
Requires-Dist: sentence-transformers>=2.2.2
Requires-Dist: faiss-cpu>=1.11.0
Requires-Dist: torch>=2.2

# IRE-RAG-LLM
##  Version 1.1.1 ‚Äî Conversational RAG (LangGraph)

| Feature | v1.0.0 (single-turn) | v1.1.1 |
|---------|------------------|-----------------------|
| Retrieval store | FAISS index of Lilian Weng blog | same index  |
| LLM | any OpenAI / Gemini | **Local Ollama (default `mistral:7b`)** |
| Memory | none | **Conversational context** via LangGraph |
| API | GET `/retrieve` | **POST `/chat`** (multi-turn) |

### Quick start

```bash
git clone ...
cd ire-rag-llm
python -m venv .venv && source .venv/Scripts/activate  # Win: Activate

pip install -e .            # installs v2 deps
python scripts/build_index.py   # one-time: downloads & indexes the blog post

ollama serve &               # make sure Mistral-7B is pulled
uvicorn src.rag.app:app --reload --port 8000
```

## Then:

    curl -X POST http://localhost:8000/chat \
         
         -H "Content-Type: application/json" \
         
         -d '{"history": [], "message": "What is task decomposition?"}'

## RAM note
    mistral:7b needs >=4.2 GiB free.
    
    If that‚Äôs tight, pull llama3:8b-instruct-q4_K_M (you need a model that has tools-enabled) and
    
    change one line in src/rag/conversational_chain.py:

        "llm = ChatOllama(model="llama3:8b-instruct-q4_K_M", temperature=0.2)"

## Offline Run(hf + ollama)
```bash
    "No cloud keys, no OpenAI fees, everything happens on your laptop."
```
## 1.  Install the extra wheels
```bash
    pip install -U langchain-ollama langchain-huggingface ollama
```
## 2. use a tools-enabled model based on your memory size
```bash
    ollama pull mistral:7b-instruct
```
## 3. Build the FAISS index once
```bash
    python scripts/build_index.py

```
    This downloads Lilian Weng‚Äôs RAG blog post, chunks it, embeds with the free
    
    sentence-transformers/all-MiniLM-L6-v2 model and writes
    
    data/faiss_blog/.

## 4. Start the lacal ollama
```bash
    ollama serve &   # keep running in a seperate tab
    uvicorn src.rag.app:app - reload --port 8000
```
## 5. Now you can use it
```bash
    # one-off curl
    curl -X POST http://localhost:8000/chat \
    
         -H "Content-Type: application/json" \
    
         -d '{"history": [], "message": "Hi üëã"}'

    # two-turn example
    curl -X POST http://localhost:8000/chat \
        
         -H "Content-Type: application/json" \
        
         -d '{"history": [{"role":"user","content":"Hi"}, {"role":"assistant","content":"Hello! How can I help?"}], "message": "What is task decomposition?"}'
```
    Ollama streams the tokens in the terminal running uvicorn;  
    
    curl returns a compact JSON answer:
```bash
    {
        "answer": "Task decomposition means breaking a complex objective into smaller actions so an autonomous agent can solve each one in turn..."
    }
```

---

## Command-line chat

If you prefer a zero-network terminal chat, use the built-in CLI.

### Run

```bash
    python src/rag/chat_cli.py

    After running: "pip install -e ."

    run: rag-chat
```

## Example of a session
```bash
        Conversational RAG (Ctrl-C to quit)
    you ‚ñ∏ Hi
    rag ‚ñ∏ Hello! How can I help?

    you ‚ñ∏ What is task decomposition?
    rag ‚ñ∏ Task decomposition is the practice of breaking a complex goal ‚Ä¶
```

## V1.0.0 README below
    Legacy single-turn RAG scripts now live in src/rag/legacy/; the main package contains only the v2 conversational pipeline.

### V1.0.0

A local Retrieval-Augmented Generation (RAG) demo using FAISS + HuggingFace embeddings (free) and an optional LLM-API pipeline.  
Supports:
- `scripts/ingest.py` -> load & chunk HTML/TXT in `data/raw/` -> build FAISS index  
- `run-retrieval` CLI -> pure-retrieval over FAISS  
- `run-rag` CLI -> full RAG QA using prompt hub  
- FastAPI -> REST endpoint at `GET /qa?question=...&k=...`

---
## Install

```bash
"git clone https://github.com/adetuire/ire-rag-llm.git"
cd ire-rag-llm

# create virtualenv
"python -m venv .venv" # or python3 -m venv .venv
# Windows: 
"source .venv/Scripts/activate"
# macOS/Linux: 
"source .venv/bin/activate"

# installs the package & console scripts
#for editable install:
"pip install -e ."
# or "pip install -r requirements.txt"

# build the vector store once
# downloads & indexes the blog post
"python scripts/ingest.py"   

# ask something
run-rag --question "What is Task Decomposition?" --k 3

# exit the virtual environment
(.venv) $ deactivate
```

## Prepare & Index
1.  Drop any .html, .md, or .txt docs into data/raw/.

2.  Run the ingester:
        
        "python scripts/ingest.py"

    This will:

        Parse each file with BeautifulSoup

        Chunk with RecursiveCharacterTextSplitter

        Embed with HuggingFaceEmbeddings

        Build & save data/faiss_index.faiss.

3.  Run the ingester with "python scripts/ingester2.py"
    
    This will:

        Asuume you have dropped a .html/.md/.txt file in data/raw/ 
        
        or you have ran another version of ingest that populates that folder with the needed files. 
        
        If the folder is empty it raises the "No documents found"

4.  Run the ingester with "python scripts/ingest3.py"
    
    This will:

        pull Lilian Weng‚Äôs ‚ÄúLLM-Powered Autonomous Agents‚Äù blog post directly from the web (no files needed in data/raw/);

        chunks it, embeds with Sentence-Transformers (all-MiniLM-L6-v2), and saves:

            data/faiss_index.faiss

            data/store.pkl

        prints a quick progress summary.  


## Pure Retrieval CLI
Query the FAISS index without hitting any paid API:
    
    "run-retrieval --question "Explain task decomposition" --k 4"


## RAG QA CLI
If you‚Äôve built the FAISS index and have an API key in your environment (OpenAI or Google), you can run full RAG:
    
    "run-rag --question "What is Task Decomposition?" --k 3"

## FastAPI Server
Start a local server (defaults to port 8000):
    
    # "uvicorn rag.app:app --reload --port 8000"
    
    # GET /retrieve?question=&k= ‚Äì top-k chunks

    # POST /rag {"question": "...", "k": 4} ‚Äì RAG answer

## Endpoints
    GET /retrieve?q=...&k=... -> returns top-k chunks

    e.g after running "uvicorn rag.app:app --reload --port 8000";

    "http://xxx.x.x.x:8000/retrieve?q=Explain%20Task%20Decomposition&k=3"
    
    POST /rag with JSON {"q": "...", "k": 3} -> returns RAG answer

## Configuration
Create a .env alongside pyproject.toml.

.env file in project root (ignored by git) can hold:

    "OPENAI_API_KEY=..."
    "GOOGLE_API_KEY=..."

.gitignore already excludes .env, .venv/, cache, data outputs, etc.



---
### Script Guide & Quick-start Cheatsheet

| Script / CLI entry-point | What it does | required before running | Example |
| ------------------------ | ------------ | ------------------- | ------- |
| `python scripts/ingest.py` | Index **any local `.html / .md / .txt` files** in `data/raw/` into `data/faiss_index.faiss` + `data/store.pkl`. | Put files in `data/raw/` first. No API keys. | `python scripts/ingest.py` |
| `python scripts/ingest2.py` | **One-shot downloader**: fetches Lilian Weng‚Äôs ‚ÄúLLM Agents‚Äù blog post, chunks it and builds `data/faiss_index_v2.faiss`. | Internet connection. No API keys. | `python scripts/ingest2.py` |
| `python scripts/ingest3.py` | Same as above but uses a manual FAISS build + pickles the doc-store (`data/faiss_index_v3.faiss`, `data/store.pkl`). | Internet. No API keys. | `python scripts/ingest3.py` |
| `python -m src.rag.chain -q "..." [--k N]` | **Pure retrieval v1** ‚Äì returns the *k* nearest chunks from `faiss_index.faiss`. | Run **`ingest.py`** first. | `python -m src.rag.chain -q "Explain X" --k 3` |
| `python -m src.rag.chain2 -q "..." [--k N]` | **Pure retrieval v2** ‚Äì uses `faiss_index_v2.faiss`. | Run **`ingest2.py`** first. | `python -m src.rag.chain2 -q "Explain X"` |
| `run-retrieval -q "..." [-k N]` | CLI wrapper for **`chain2.retrieve_v2()`** (identical output). Installed automatically via `pip install -e .`. | Same as above. | `run-retrieval -q "Explain X" -k 5` |
| `run-rag -q "..."` | Full **RAG pipeline** ‚Äì query-analysis to filtered retrieval to LLM answer (LangGraph). Uses `chain_rag.py`. | 1. **`OPENAI_API_KEY` _or_ `GOOGLE_API_KEY`** in env  <br>2. `ingest.py` or `ingest3.py` done. | `export OPENAI_API_KEY=‚Ä¶`<br>`run-rag -q "What is Task Decomposition?"` |
| `uvicorn rag.app:app --reload --port 8000` | Spins up a **FastAPI** micro-service.<br>GET `/retrieve?q=<question>&k=<int>` returns JSON with top-k chunks (powered by `chain2`). | `ingest2.py` done. | `curl "http://localhost:8000/retrieve?q=Explain+X&k=3"` |

### Environment variables

* `OPENAI_API_KEY` ‚Äì set for OpenAI Chat completion in `chain_rag.py`.
* `GOOGLE_API_KEY` ‚Äì alternative: Gemini 2.5 Pro via `langchain-google-genai`.
* `USER_AGENT` ‚Äì polite header for web scrapers (scripts set a default).
---
