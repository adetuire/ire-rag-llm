# IRE-RAG-LLM

A local Retrieval-Augmented Generation (RAG) demo using FAISS + HuggingFace embeddings (free) and an optional LLM-API pipeline.  
Supports:
- `scripts/ingest.py` -> load & chunk HTML/TXT in `data/raw/` -> build FAISS index  
- `run-retrieval` CLI -> pure-retrieval over FAISS  
- `run-rag` CLI -> full RAG QA using prompt hub  
- FastAPI -> REST endpoint at `GET /qa?question=...&k=...`

---

## Install

```bash
"git clone https://github.com/adetuire/ire-rag-llm.git"
cd ire-rag-llm

# create virtualenv
"python -m venv .venv" # or python3 -m venv .venv
# Windows: 
".venv\Scripts\activate"
# macOS/Linux: 
"source .venv/bin/activate"

# installs the package & console scripts
"pip install -r requirements.txt"
# or for editable install:
# "pip install -e ."

# build the vector store once
# downloads & indexes the blog post
"python scripts/ingest.py"   

# ask something
run-rag --question "What is Task Decomposition?" --k 3

# exit the virtual environment
"(.venv) $ deactivate"


```

## Prepare & Index
1.  Drop any .html, .md, or .txt docs into data/raw/.

2.  Run the ingester:
        "python scripts/ingest.py"
    This will:

        Parse each file with BeautifulSoup

        Chunk with RecursiveCharacterTextSplitter

        Embed with HuggingFaceEmbeddings

        Build & save data/faiss_index.faiss


## Pure Retrieval CLI
Query the FAISS index without hitting any paid API:
    "run-retrieval --question "Explain task decomposition" --k 4"


## RAG QA CLI
If you’ve built the FAISS index and have an API key in your environment (OpenAI or Google), you can run full RAG:
    "run-rag --question "What is Task Decomposition?" --k 3"

## FastAPI Server
Start a local server (defaults to port 8000):
    "uvicorn rag.app:app --reload --port 8000"
    # GET /retrieve?question=&k= – top-k chunks

    # POST /rag {"question": "...", "k": 4} – RAG answer

Endpoints
    GET /retrieve?question=...&k=... -> returns top-k chunks
    POST /rag with JSON {"question": "...", "k": 3} -> returns RAG answer

## Configuration
Create a .env alongside pyproject.toml.
.env file in project root (ignored by git) can hold:
    "OPENAI_API_KEY=..."
    "GOOGLE_API_KEY=..."
.gitignore already excludes .env, .venv/, cache, data outputs, etc.